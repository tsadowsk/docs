---
title: Getting started
weight: 10
aliases: /multicloud-gitops-sgx/mcg-sgx-getting-started/
---

include::modules/comm-attributes.adoc[]

:toc:
:imagesdir: /images
:_content-type: ASSEMBLY

[id="deploying-mcg-pattern"]
== Deploying the {sgx-mcg-pattern}

.Prerequisites

* An OpenShift cluster
 ** To create an OpenShift cluster, go to the https://console.redhat.com/[Red Hat Hybrid Cloud console] and select *Services \-> Containers \-> Create cluster*.
 ** The cluster must have a dynamic `StorageClass` to provision `PersistentVolumes`.
 ** link:../../multicloud-gitops-sgx/mcg-sgx-cluster-sizing[Cluster sizing] requirements.
* Optional: A second OpenShift cluster for multicloud demonstration.
//Replaced git and podman prereqs with the tooling dependencies page
* https://validatedpatterns.io/learn/quickstart/[Install the tooling dependencies].

The use of this pattern depends on having at least one running Red Hat OpenShift cluster. However, consider creating a cluster for deploying the GitOps management hub assets and a separate cluster for the managed cluster.

If you do not have a running Red Hat OpenShift cluster, you can start one on a
public or private cloud by using https://console.redhat.com/openshift/create[Red Hat Hybrid Cloud Console].

.Procedure

. https://validatedpatterns.io/learn/quickstart/[Install the tooling dependencies].
+
//[ii]remember to give proper links!!!
. Fork the https://github.com/validatedpatterns-sandbox/sgx-accelerated-multicloud-gitops[multicloud-gitops-sgx] repository on GitHub.
. Clone the forked copy of this repository.
+
[source,terminal]
----
git clone git@github.com:your-username/sgx-accelerated-multicloud-gitops.git
----

. Create a local copy of the secret values file that can safely include credentials for the config-demo application and edit it if you want to customize the secret. If not, the framework generates a random password.
+
[source,terminal]
----
cp values-secret.yaml.template ~/values-secret-multicloud-gitops.yaml
----
+
[WARNING]
====
Do not commit this file. You do not want to push personal credentials to GitHub.
====

. (Optional) You may customize the deployment for your cluster depending on your needs by editing values-global.yaml and values-hub.yaml. To do this run the following commands:
+
[source,terminal]
----
git checkout -b my-branch
----
+
[source,terminal]
----
vi values-global.yaml
----
+
[source,terminal]
----
git add values-global.yaml
----
+
[source,terminal]
----
git commit values-global.yaml
----
+
[source,terminal]
----
git push origin my-branch
----

. Prepare and build Vault docker image and convert it to SGX using Gramine Shielded Containers (gsc).

. Deploy the pattern by running `./pattern.sh make install` or by using the link:/infrastructure/using-validated-pattern-operator/[Validated Patterns Operator] - both methods are described below.



























Prerequisites:

Gramine Shielded Containres application requires a machine with the same OS as input converted image in order to create SGX convered image.
In this case it would be Red Hat Enterprise Linux 8.8
This machine doesn't have to have SGX feature enabled becuase it is used for conversion process only.





mkdir vault

cat Dockerfile-vault-no-docker-entrypoint-and-dev


Create modified Vault docker file which will be input for SGX conversion process:
FROM registry.connect.redhat.com/hashicorp/vault:1.15.3-ubi
ENTRYPOINT ["vault"]

Build the Vault docker image:
docker build -t seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-no-sgx -f Dockerfile-vault-no-docker-entrypoint-and-dev .










--------------------------------------------------------------------


Install Prerequisites according to:
https://gramine.readthedocs.io/projects/gsc/en/latest/#prerequisites



git clone https://github.com/gramineproject/gsc.git

cs gsc





---

https://gramine.readthedocs.io/projects/gsc/en/latest/#example








cp config.yaml.template config.yaml

# Specify the OS distro that is used to build Gramine, i.e., the distro from where the Gramine build
# gets all tools and dependencies from. This distro should match the distro underlying the
# application's Docker image; otherwise the results may be unpredictable (if you specify `"auto"`,
# which is recommended, you don't need to worry about the mismatch).
#
# Currently supported distros are:
# - ubuntu:20.04, ubuntu:21.04, ubuntu:22.04, ubuntu:23.04
# - debian:10, debian:11, debian:12
# - centos:8
# - redhat/ubi8:8.8
# - redhat/ubi8-minimal:8.8

# If Distro is set to "auto", GSC detects the distro automatically by examining the supplied
# Docker image. Alternatively, Distro can be set to one of the supported distros mentioned above.
Distro: "auto"

# If the image has a specific registry, define it here.
# Empty by default; example value: "registry.access.redhat.com/ubi8".
Registry: ""

# If you're using your own fork and branch of Gramine, specify the GitHub link and the branch name
# below; typically, you want to keep the default values though.
#
# It is also possible to specify the prebuilt Gramine Docker image (that was built previously via
# the `gsc build-gramine` command). For this, remove Repository and Branch and instead write:
#   Image:      "<prebuilt Gramine Docker image>"
#
# GSC releases are guaranteed to work with corresponding Gramine releases (and GSC `master`
# branch is guaranteed to work with current Gramine `master` branch).
Gramine:
    Repository: "https://github.com/gramineproject/gramine.git"
    Branch:     "master"

# Specify the Intel SGX driver installed on your machine (more specifically, on the machine where
# the graminized Docker container will run); there are several variants of the SGX driver:
#
#   - upstream (in-kernel) driver: use empty values like below
#         Repository: ""
#         Branch:     ""
#
#   - DCAP out-of-tree driver: same as above, use empty values
#         Repository: ""
#         Branch:     ""
#
#   - legacy out-of-tree driver: use something like the below values, but adjust the branch name
#         Repository: "https://github.com/01org/linux-sgx-driver.git"
#         Branch:     "sgx_driver_1.9"
#
SGXDriver:
    Repository: ""
    Branch:     ""


---

create:

vault.manifest

sgx.enclave_size = "2G"
#sgx.max_threads = 16
sgx.max_threads = 512

sgx.allowed_files = [
  "file:/vault/",
  "file:/vault/data/",
  "file:/vault/data/core/",
  "file:/vault/data/core/_seal-config",
  "file:/vault/config/",
  "file:/home/vault/",
  "file:/etc/security/pam_env.conf",
  "file:/etc/security/limits.conf",
  "file:/var/log/lastlog",
]

sgx.debug = true

loader.uid = 100
loader.gid = 107

fs.mounts = [
    { path = "/vault/data", uri = "file:/vault/data", type = "encrypted", key_name = "default" },
]
fs.insecure__keys.default = "ffeeddccbbaa99887766554433221100"

loader.insecure__use_host_env = true
loader.insecure__use_cmdline_argv = true


---


openssl genrsa -3 -out enclave-key.pem 3072




./gsc build --insecure-args seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-no-sgx vault.manifest     <- SECURE ARGS ARE IN THE EXAMPLE TOO




(Optional) To convert Vault image behind proxy use below command line:
./gsc build --insecure-args \
       --build-arg no_proxy=10.105.195.132/24,.intel.com,localhost,127.0.0.1,.svc,.svc.cluster.local,.nip.io \
       --build-arg https_proxy=http://proxy-fm.intel.com:911 \
       --build-arg NO_PROXY=10.105.195.132/24,.intel.com,localhost,127.0.0.1,.svc,.svc.cluster.local,.nip.io \
       --build-arg HTTPS_PROXY=http://proxy-fm.intel.com:911 \
       --build-arg HTTP_PROXY=http://proxy-fm.intel.com:911 \
       --build-arg http_proxy=http://proxy-fm.intel.com:911 \
       seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-no-sgx vault.manifest

HOW TO GENERATE PEM KEY FILE?????



./gsc sign-image seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-no-sgx enclave-key.pem

docker tag gsc-seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-no-sgx seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-sgx-gramine
docker login -u=seahorse910 docker.io 
docker push seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-sgx-gramine  






docker.io/seahorse910/sgx-gramine:rh-vault-no-docker-entrypoint-sgx-gramine



























[id="biuld-docker-image-and-convert-to-sgx"]
== Prepare Vault docker image and convert to SGX

To deploy the cluster by using the `pattern.sh` file, complete the following steps:

. Login to your cluster by running the following command:
+
[source,terminal]
----
oc login
----
+
Optional: Set the `KUBECONFIG` variable for the `kubeconfig` file path:
+
[source,terminal]
----
export KUBECONFIG=~/<path_to_kubeconfig>
----

. Deploy the pattern to your cluster. Run the following command:
+
[source,terminal]
----
./pattern.sh make install
----

Verify that the Operators have been installed.

























[id="deploying-cluster-using-patternsh-file"]
== Deploying the cluster by using the pattern.sh file

To deploy the cluster by using the `pattern.sh` file, complete the following steps:

. Login to your cluster by running the following command:
+
[source,terminal]
----
oc login
----
+
Optional: Set the `KUBECONFIG` variable for the `kubeconfig` file path:
+
[source,terminal]
----
export KUBECONFIG=~/<path_to_kubeconfig>
----

. Deploy the pattern to your cluster. Run the following command:
+
[source,terminal]
----
./pattern.sh make install
----

Verify that the Operators have been installed.

. To verify, in the *OpenShift Container Platform* web console, navigate to *Operators â†’ Installed Operators* page.
. Check that the following Operators are installed with  `Succeeded` status (Figure 1): 
* *Advanced Cluster Management for Kubernetes* 
* *multicluster engine for Kubernetes* 
* *Node Feature Discovery Operator* 
* *Red Hat Openshift GitOps*
* *Validated Patterns Operator*
+
.List of Installed Operators for Multicloud GitOps Validated Pattern with SGX
image::multicloud-gitops-sgx/sgx-installed-operators.png[Multicloud GitOps Hub with SGX installed operators,role="related thumb right"]
+


== Deploying the cluster by using the Validated Patterns Operator

To install the *Validated Patterns Operator*:

. Log in to the *Openshift Container Platform* web console and select *Operators > OperatorHub*.

. Search for *Validated Patterns Operator*, open it and click *Install*.
+
.Instal Validated Patterns Operator
image::multicloud-gitops-sgx/sgx-validated-patter-operator.png[Install Validated Patterns Operator,scale=50]
+
. Choose default settings for the installation mode, namespaces and update strategy and confirm it by clicking *Install*.

. Select *Operators > Installed Operators*.

. Ensure that *Validated Patterns Operator* is listed in the `openshift-operators` project with a status `Succeeded`.

After succeeded installation open *Validated Patterns Operator*, go to *Pattern* tab and click *Create Pattern*.

. Fill the *Name* of the pattern `multicloud-gitops-sgx` and *Cluster Group Name* `hub` (from _values-global.yaml_ file).

. Under *Git Config > Target Repo* copy the link to your fork and under *Git Config > Target Revision* write the name of your branch (Figure 3).

. Click *Create* to create the pattern.
+
.Create Pattern Form
image::multicloud-gitops-sgx/sgx-create-pattern.png[Create pattern Multicloud GitOps with SGX]
+


Verify that the rest of Operators have been installed:

. To verify, in the *OpenShift Container Platform* web console, navigate to *Operators â†’ Installed Operators* page.
. Check that the following Operators are installed with  `Succeeded` status (Figure 1): 
* *Advanced Cluster Management for Kubernetes* 
* *multicluster engine for Kubernetes* 
* *Node Feature Discovery Operator* 
* *Red Hat Openshift GitOps*

Add a secret for `config-demo` application (from _values-secret-multicloud-gitops.yaml_) to *Vault* manually:

. Go to Vault service route. URL can be found:

.. by running command:
+
[source,terminal]
----
oc -n vault get route vault -ojsonpath='{.spec.host}'
----
+
.. in *Openshift Container Platform* web console under *Networking > Routes* for `vault` project.

. Log into the Vault using root token. Root token can be found by executing command:
+
[source,terminal]
----
oc -n imperative get secrets vaultkeys -ojsonpath='{.data.vault_data_json}' | base64 -d
----
+

. After login go to `secret` catalog and clik *Create secret* and fill all the fields manually (Figure 2):

.. *Path for this secret* is `global/config-demo` (from _values.yaml_ file for `config-demo` charts)

.. Under *Secret data* key is `secret` (from _values-secret-multicloud-gitops.yaml_ file) and in next field put its value.

.. Click *Add* and then *Save*.
+
.Create secret
image::multicloud-gitops-sgx/sgx-secret-vault.png[Create secret in the vault]
+


== Verification

Go to the Hub ArgoCD and verify that all applications are synchronized. The URL can be found in *Openshift Container Platform* web console under *Networking > Routes* for the project `multicloud-gitops-sgx-hub` or use command:

[source,terminal]
----
oc -n multicloud-gitops-sgx-hub get route hub-gitops-server -ojsonpath='{.spec.host}'
----

All applications should be `Healthy` and `Synced`:

.ArgoCD panel with `sgx-app`
image::multicloud-gitops-sgx/multicloud-gitops-argocd-sgx.png[Multicloud GitOps Hub with `sgx-app`]

Check the logs of a pod `sgx-app` to verify if it uses *{intel-sgx}*. In the *OpenShift Container Platform* web console, navigate to *Workloads > Pods*. Change project to `sgx-app` and open the *Logs* tab in the pod details. The appearance of avx_512_core_sgx_bf16 flag on the list of compiled instructions confirms that {intel-sgx} is used.

As part of this pattern, HashiCorp Vault has been installed. Refer to the section on https://validatedpatterns.io/secrets/vault/[Vault].

[id="next-steps_mcg-getting-started"]
== Next steps

After the management hub is set up and works correctly, attach one or more managed clusters to the architecture.

For instructions on deploying the edge, refer to link:../mcg-sgx-managed-cluster/[Attach a managed cluster (edge) to the management hub].

//For instructions on deploying the edge, refer to xref:/multicloud-gitops/mcg-managed-cluster.adoc#attach-managed-cluster[Attach a managed cluster (edge) to the management hub].
